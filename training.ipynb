{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files loaded: 16599\n",
      "Number of files loaded: 800\n"
     ]
    }
   ],
   "source": [
    "# train_cpu.ipynb\n",
    "\n",
    "# 1. Importing required libraries\n",
    "import os\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from networks_unetpp.Networks import UNetPlusPlus \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from dataset.landslide_dataset import LandslideDataSet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the eval_image function\n",
    "def eval_image(pred, label, num_classes):\n",
    "    TP = np.zeros((num_classes, 1))\n",
    "    FP = np.zeros((num_classes, 1))\n",
    "    TN = np.zeros((num_classes, 1))\n",
    "    FN = np.zeros((num_classes, 1))\n",
    "    n_valid_sample = 0\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        TP[i] = np.sum((pred == i) & (label == i))\n",
    "        FP[i] = np.sum((pred == i) & (label != i))\n",
    "        TN[i] = np.sum((pred != i) & (label != i))\n",
    "        FN[i] = np.sum((pred != i) & (label == i))\n",
    "        n_valid_sample += np.sum(label == i)\n",
    "\n",
    "    return TP, FP, TN, FN, n_valid_sample\n",
    "\n",
    "# 2. Defining the necessary configurations and settings\n",
    "data_dir = './dataset/'  # Directory where your dataset is located\n",
    "train_list = './dataset/train.txt'  # Path to the train.txt file\n",
    "test_list = './dataset/test.txt'  # Path to the test.txt file\n",
    "input_size = '128,128'  # Input size for the images\n",
    "num_classes = 2  # Number of classes (Landslide, Non-Landslide)\n",
    "batch_size = 32  # Batch size for training\n",
    "num_workers = 4  # Number of workers for data loading\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "num_steps = 400  # Number of training steps\n",
    "num_steps_stop = 400  # Number of training steps for early stopping\n",
    "weight_decay = 5e-4  # Weight decay for regularization\n",
    "snapshot_dir = './snapshots/'  # Directory to save model snapshots\n",
    "\n",
    "# Ensure that the snapshot directory exists\n",
    "if not os.path.exists(snapshot_dir):\n",
    "    os.makedirs(snapshot_dir)\n",
    "\n",
    "# 3. Loading the data\n",
    "def load_data():\n",
    "    src_loader = data.DataLoader(\n",
    "        LandslideDataSet(data_dir, train_list, max_iters=num_steps_stop * batch_size, set='labeled'),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    test_loader = data.DataLoader(\n",
    "        LandslideDataSet(data_dir, test_list, set='unlabeled'),\n",
    "        batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return src_loader, test_loader\n",
    "\n",
    "src_loader, test_loader = load_data()\n",
    "\n",
    "# 4. Initializing the U-Net model\n",
    "model = UNetPlusPlus(n_classes=num_classes)\n",
    "\n",
    "# 5. Defining the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Training function\n",
    "def train_model(num_steps_stop):\n",
    "    hist = np.zeros((num_steps_stop, 3))\n",
    "    for batch_id, src_data in enumerate(src_loader):\n",
    "        if batch_id == num_steps_stop:\n",
    "            break\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, labels, _, _ = src_data\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        labels = labels.long()\n",
    "        loss = cross_entropy_loss(pred, labels)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate batch accuracy\n",
    "        _, predicted_labels = torch.max(pred, 1)\n",
    "        predicted_labels = predicted_labels.detach().cpu().numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        batch_oa = np.sum(predicted_labels == labels) * 1.0 / len(labels.reshape(-1))\n",
    "        hist[batch_id, 0] = loss.item()\n",
    "        hist[batch_id, 1] = batch_oa\n",
    "        hist[batch_id, -1] = time.time() - start_time\n",
    "\n",
    "        # Print progress\n",
    "        if (batch_id + 1) % 10 == 0:\n",
    "            print(f'Iter {batch_id+1}/{num_steps} Time: {np.mean(hist[batch_id-9:batch_id+1,-1]):.2f} Batch_OA = {np.mean(hist[batch_id-9:batch_id+1,1])*100:.1f} cross_entropy_loss = {np.mean(hist[batch_id-9:batch_id+1,0]):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/400 Time: 46.18 Batch_OA = 83.5 cross_entropy_loss = 0.493\n",
      "Iter 20/400 Time: 43.79 Batch_OA = 98.1 cross_entropy_loss = 0.270\n",
      "Iter 30/400 Time: 53.94 Batch_OA = 98.0 cross_entropy_loss = 0.202\n",
      "Iter 40/400 Time: 55.33 Batch_OA = 98.3 cross_entropy_loss = 0.151\n",
      "Iter 50/400 Time: 48.26 Batch_OA = 98.6 cross_entropy_loss = 0.120\n",
      "Iter 60/400 Time: 45.77 Batch_OA = 98.4 cross_entropy_loss = 0.101\n",
      "Iter 70/400 Time: 47.68 Batch_OA = 97.9 cross_entropy_loss = 0.093\n",
      "Iter 80/400 Time: 47.17 Batch_OA = 98.6 cross_entropy_loss = 0.074\n",
      "Iter 90/400 Time: 47.47 Batch_OA = 98.3 cross_entropy_loss = 0.071\n",
      "Iter 100/400 Time: 47.16 Batch_OA = 98.2 cross_entropy_loss = 0.070\n",
      "Iter 110/400 Time: 47.16 Batch_OA = 98.6 cross_entropy_loss = 0.058\n",
      "Iter 120/400 Time: 47.08 Batch_OA = 98.4 cross_entropy_loss = 0.058\n",
      "Iter 130/400 Time: 47.09 Batch_OA = 98.3 cross_entropy_loss = 0.058\n",
      "Iter 140/400 Time: 47.17 Batch_OA = 98.5 cross_entropy_loss = 0.050\n",
      "Iter 150/400 Time: 47.08 Batch_OA = 98.8 cross_entropy_loss = 0.045\n",
      "Iter 160/400 Time: 47.25 Batch_OA = 98.5 cross_entropy_loss = 0.048\n",
      "Iter 170/400 Time: 47.03 Batch_OA = 98.5 cross_entropy_loss = 0.049\n",
      "Iter 180/400 Time: 47.25 Batch_OA = 98.1 cross_entropy_loss = 0.055\n",
      "Iter 190/400 Time: 47.17 Batch_OA = 98.5 cross_entropy_loss = 0.046\n",
      "Iter 200/400 Time: 47.25 Batch_OA = 98.7 cross_entropy_loss = 0.041\n",
      "Iter 210/400 Time: 47.41 Batch_OA = 98.5 cross_entropy_loss = 0.045\n",
      "Iter 220/400 Time: 47.43 Batch_OA = 98.8 cross_entropy_loss = 0.039\n",
      "Iter 230/400 Time: 46.38 Batch_OA = 98.6 cross_entropy_loss = 0.043\n",
      "Iter 240/400 Time: 51.25 Batch_OA = 98.7 cross_entropy_loss = 0.038\n",
      "Iter 250/400 Time: 53.84 Batch_OA = 98.6 cross_entropy_loss = 0.039\n",
      "Iter 260/400 Time: 54.42 Batch_OA = 98.5 cross_entropy_loss = 0.042\n",
      "Iter 270/400 Time: 47.71 Batch_OA = 98.5 cross_entropy_loss = 0.043\n",
      "Iter 280/400 Time: 51.39 Batch_OA = 98.7 cross_entropy_loss = 0.037\n",
      "Iter 290/400 Time: 51.75 Batch_OA = 98.5 cross_entropy_loss = 0.041\n",
      "Iter 300/400 Time: 54.13 Batch_OA = 98.7 cross_entropy_loss = 0.037\n",
      "Iter 310/400 Time: 52.90 Batch_OA = 98.4 cross_entropy_loss = 0.044\n",
      "Iter 320/400 Time: 51.08 Batch_OA = 98.1 cross_entropy_loss = 0.051\n",
      "Iter 330/400 Time: 55.26 Batch_OA = 98.4 cross_entropy_loss = 0.043\n",
      "Iter 340/400 Time: 52.70 Batch_OA = 98.8 cross_entropy_loss = 0.034\n",
      "Iter 350/400 Time: 57.64 Batch_OA = 98.5 cross_entropy_loss = 0.040\n",
      "Iter 360/400 Time: 61.11 Batch_OA = 98.4 cross_entropy_loss = 0.043\n",
      "Iter 370/400 Time: 61.40 Batch_OA = 98.9 cross_entropy_loss = 0.031\n",
      "Iter 380/400 Time: 50.90 Batch_OA = 98.6 cross_entropy_loss = 0.038\n",
      "Iter 390/400 Time: 44.32 Batch_OA = 98.4 cross_entropy_loss = 0.043\n",
      "Iter 400/400 Time: 44.06 Batch_OA = 98.6 cross_entropy_loss = 0.039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Run training and testing\n",
    "train_model(num_steps_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./snapshots/unet_plus_plus_trained_model1.pth\n"
     ]
    }
   ],
   "source": [
    "# 7. Saving the Trained Model\n",
    "def save_model(model, snapshot_dir, model_name='unet_plus_plus_trained_model1.pth'):\n",
    "    # Ensure the snapshot directory exists\n",
    "    if not os.path.exists(snapshot_dir):\n",
    "        os.makedirs(snapshot_dir)\n",
    "    \n",
    "    # Save the model's state_dict (recommended way of saving models)\n",
    "    model_path = os.path.join(snapshot_dir, model_name)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "# Example usage after your training loop:\n",
    "save_model(model, snapshot_dir, 'unet_plus_plus_trained_model1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
